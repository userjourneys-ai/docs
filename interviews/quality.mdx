---
title: "Quality Evaluation"
description: "How interview quality is automatically evaluated and what it means for your insights."
---

Not all interviews provide useful feedback. The system automatically evaluates each interview as **Successful** or **Unsuccessful** based on the quality of responses.

## What Makes an Interview Unsuccessful

- **Very short duration** — Interviews under 2 minutes typically lack substance
- **Off-topic discussion** — User didn't address your questions
- **Technical issues** — Audio problems or disconnections interrupted the conversation
- **One-word answers** — Only brief, surface-level responses throughout
- **Uncooperative behavior** — User was joking, testing the system, or not engaging genuinely

## How Quality Affects Results

| Status | Default Visibility | Included in Insights |
|--------|-------------------|---------------------|
| **Successful** | Shown | Yes |
| **Unsuccessful** | Hidden | No |

Unsuccessful interviews are excluded from [insights](/interviews/insights) to prevent low-quality responses from skewing your patterns.

## Viewing Low-Quality Interviews

To see unsuccessful interviews and understand why they failed:

1. Open the experiment dropdown
2. Toggle "Show all responses"
3. Review transcripts to understand what went wrong

<Tip>
If you're seeing many unsuccessful interviews, review your experiment setup. Clearer [questions](/interviews/questions), better [product context](/interviews/experiments#product-context), or a different audience might help.
</Tip>
